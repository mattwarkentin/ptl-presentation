---
title: "PyTorch Lightning: Deep Learning API for PyTorch"
subtitle: "Campbell Lab Meeting: Tech Talk"
author: 
  - Matt Warkentin
institute: "Lunenfeld-Tanenbaum Research Institute, .b[.sinai-blue[Sinai] .sinai-orange[Health] .sinai-red[System]]"
date: 'June 10th, 2022'
always_allow_html: true
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: 
      - css/xaringan-theme.css
      - css/style.css
    seal: false
    nature:
      titleSlideClass: ["bottom", "left", "hide-count"]
      slideNumberFormat: "%current%"
      highlightStyle: atom-one-light
      highlightLines: true
      ratio: 16:9
      countIncrementalSlides: true
      navigation:
        scroll: false
---

name: title
class: left middle hide-count title-bg

```{r setup, echo=FALSE, message=FALSE, warning=FALSE}
source("xaringan-theme.R")
source("setup.R")$value
library(htmltools)
library(glue)
```

```{r title-slide, echo=FALSE, message = FALSE, warning = FALSE}
ids <-
  c(
    fontawesome::fa('video', fill = '#FFFFFF')
  )
htmltools::withTags(
  div(
    class = "talk-meta",
    div(
      p(rmarkdown::metadata$title, class = "talk-title"),
      p(rmarkdown::metadata$subtitle, class = "talk-subtitle")
    ),
    div(
      class = "talk-author",
      glue_collapse(
        glue("<span>{ids} {rmarkdown::metadata$author}</span>"), 
        sep = "<br>"
      ),
      br(), 
      br(),
      div(
        fontawesome::fa('home', fill = '#FFFFFF'),
        rmarkdown::metadata$institute
        )
    ),
    div(
      class = "talk-date",
      span(
        fontawesome::fa('calendar-alt', fill = '#FFFFFF'),
        rmarkdown::metadata$date
      )
    )
  )
)
```

---

class: lh-copy

# Background

- This presentation assumes some moderate to advanced familiarity with deep learning workflows and `PyTorch`

- Briefly: `PyTorch` is an open source machine learning framework that accelerates the path from research prototyping to production deployment.
    + https://pytorch.org/

- PyTorch, like Tensorflow, is a low-level computational library

- Often, as machine learning practitioners, we may wish to use higher-level libraries that simplify or abstract away technical details that slow down getting up and running
    + Quick victories and iteration are key to successful AI projects

---

class: lh-copy

# Problems with high-level libraries

- Debatably, Keras is to Tensorflow what PyTorch Lightning/Ignite/Other are to PyTorch

- My primary issue with Keras was that it was too *hand-holdy*

- It works really well for "traditional" projects, but as soon as you needed to deviate from the anticipated workflow, there was a lot of friction

- In other words, Keras abstracted away technical debt at the cost of losing flexibility

--

- Why is **PyTorch Lightning** different?
    + PyTorch Lightning isn't an abstraction, it is PyTorch
    + Anything that can be achieved with vanilla PyTorch can be done with PTL
    + Your project can be as simple or complex as you desired, it is simply a matter of the "helper" code in PTL you want to overwrite


---

# Every deep learning project...

```py
for i in range(epochs):
  for j in range(train_batches):
    # put model in train mode
    # assemble a batch
    # forward pass
    # compute loss
    # log metrics
    # zero gradients
    # backward step
    # optimizer step
  
  for j in range(valid_batches):
    # put model in eval mode
    # assemble a batch
    # forward pass
    # compute loss
    # log metrics
```

---

# The unique part of your project...

```py
for i in range(epochs):
  for j in range(train_batches):
    # put model in train mode
    # assemble a batch
*   # forward pass
    # compute loss
    # log metrics
    # zero gradients
    # backward step
    # optimizer step
  
  for j in range(valid_batches):
    # put model in eval mode
    # assemble a batch
*   # forward pass
    # compute loss
    # log metrics
```

---

# PyTorch Lightning API

- PTL provides a class called a `LightningModule` which wraps up the logic for training a deep learning model

- As a user, your model needs to subclass the `LightningModule` class and define a small number of methods, while letting PTL handle the rest

- Requisite methods:
    + `init`
    + `forward`
    + `configure_optimizers`
    + `training_step`

- Optional, but common methods:
    + `validation_step`
    + `test_step`

---

class: lh-copy

# PyTorch Lightning API

.left-column[
#### LightningModule
]

.right-column[
```py
import pytorch_lightning as pl

class MySweetModel(pl.LightningModule):
    __init__(self, ...):
        pass
    
    forward(self, x):
        return y_hat
    
    configure_optimizers(self):
        return optimizer
    
    training_step(self, batch, batch_id):
        return train_loss
```
]

---

# PyTorch Lightning API

- While a `LightningModule` defines the logic for your deep learning model, you will also want to define a class that encapsulates the logic of your data

- Taken together, the data logic and model logic should be transportable and work in (almost) any environment

> A datamodule is a shareable, reusable class that encapsulates all the steps needed to process data

- Similar to `LightningModule`, your data module should subclass `LightningDataModule` and define some important methods

- Requisite methods:
    + `init`
    + `setup`
    + `train_dataloader`
    
- Optional: `prepare_data`, `val_dataloader`, `test_dataloader`

---

class: lh-copy

# PyTorch Lightning API

.left-column[
#### .gray[LightningModule]

#### LightningDataModule
]

.right-column[
```py
class MySweetData(pl.LightningDataModule):
    __init__(self, ...):
        pass
    
    prepare_data(self):
        # Download data
    
    setup(self, stage)
        # Train/validation/test splits
        # Define any transforms
        # Create PyTorch Dataset(s)
        
    train_dataloader(self):
        return train_dl
```
]

---

class: lh-copy

# PyTorch Lightning API

.left-column[
#### .gray[LightningModule]

#### .gray[LightningDataModule]

#### Trainer
]

.right-column[
> Once youâ€™ve organized your PyTorch code into a LightningModule, the Trainer automates everything else.

- There are many arguments that can be passed to your trainer to configure the training process

```py
trainer = pl.Trainer(
  min_epochs=10,
  max_epochs=100,
  callbacks=...,
  logger=...,
  profiler='simple'
)
```
]

---

class: lh-copy

# PyTorch Lightning API

.left-column[
#### .gray[LightningModule]

#### .gray[LightningDataModule]

#### Trainer
]

.right-column[
- Create instances of your model and data module and pass them to the `fit()` method of the `trainer` to begin the fitting process

```py
model = MySweetModel()
datamodule = MySweetData()

trainer = pl.Trainer(...)

trainer.fit(model, datamodule)

# If test logic is defined
trainer.test(model, datamodule)
```
]

---

class: lh-copy

# PyTorch Lightning API

.left-column[
#### .gray[LightningModule]

#### .gray[LightningDataModule]

#### .gray[LightningDataModule]

#### Hooks
]

.right-column[
- Hooks are simply points along the training-validation-test loop where you can "hook in" and perform some task

- That task can be basically anything you want

- The pre-built callbacks are defined using these hooks, by performing some task (e.g. saving a model checkpoint) at the right point in the process

- [Pseudocode of hooks](https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_module.html#hooks)

- [ModelHooks API](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.core.hooks.ModelHooks.html#pytorch_lightning.core.hooks.ModelHooks)
]

---

class: lh-copy

# PyTorch Lightning API

.left-column[
#### .gray[LightningModule]

#### .gray[LightningDataModule]

#### .gray[Trainer]

#### .gray[Hooks]

#### Callbacks
]

.right-column[
- More than 20 callbacks available for use

```py
cb_progress = RichProgressBar()

cb_earlystop = EarlyStopping()

cb_chkpt = ModelCheckpoint()

trainer = pl.Trainer(
  ...,
  callbacks=[cb_progress, cb_earlystop, cb_chkpt]
)

trainer.fit(model, data)
```
]

---

class: lh-copy

# PyTorch Lightning API

.left-column[
#### .gray[LightningModule]

#### .gray[LightningDataModule]

#### .gray[Trainer]

#### .gray[Hooks]

#### Callbacks
]

.right-column[
- Easily create custom callbacks by subclassing `Callbacks` and defining logic at the appropriate point in the training process

```py
class EmailMe(Callback):
    __init__(self, address):
        self.address = address
    
    on_validation_epoch_end(trainer, pl_module):
        # Some code to send me an email

cb_email = EmailMe('warkentin@lunenfeld.ca')

trainer = pl.Trainer(callbacks=[cb_email])
trainer.fit(model, data)
```
]

---

class: lh-copy

# PyTorch Lightning API

.left-column[
#### .gray[LightningModule]

#### .gray[LightningDataModule]

#### .gray[Trainer]

#### .gray[Hooks]

#### .gray[Callbacks]

#### Loggers
]

.right-column[
- Loggers are used to record something to disk for later inspection
    + Comet, CSV, MLFlow, Neptune, Tensorboard, Test Tube, Weights and Biases

```py
log_csv = CSVLogger()

log_tb = TensorBoardLogger()

trainer = pl.Trainer(
  ...,
  logger=[log_csv, log_tb]
)

trainer.fit(model, data)
```
]

---

# PyTorch Lightning API

.left-column[
#### .gray[LightningModule]

#### .gray[LightningDataModule]

#### .gray[Trainer]

#### .gray[Hooks]

#### .gray[Callbacks]

#### .gray[Loggers]

#### What else?
]

.right-column[
- So so much...

- Solid documentation/examples

- Hooks everywhere

- Callbacks
    + Early stopping
    + Model checkpoints
    + Model pruning
    + Progress bars
    + Custom? Inherit from `Callback`

- Loggers
    + CSV, Tensorboard, Weights and Biases, and more...
    
- Profiling
]

---

# PyTorch Lightning API

.left-column[
#### .gray[LightningModule]

#### .gray[LightningDataModule]

#### .gray[Trainer]

#### What else?
]

.right-column[
- Accelerators
    + CPU, GPU, HPU, IPU, TPU...
    + GPU-acceleration for M1 Macs
        + <https://github.com/PyTorchLightning/pytorch-lightning/pull/13123>
        
- Plugins for mixed precision training

- Strategies

- LightningCLI for command-line use

- Environment support for SLURM
    + Including auto-requeuing

- Or use `test-tube` to easily construct SLURM submission scripts and enable hyperparameter tuning/grid search 
]

---

class: lh-copy

# `r fontawesome::fa('question-circle')` Helpful Resources

.pull-left[
- [PyTorch Lightning Documentation](https://pytorch-lightning.readthedocs.io/en/latest/?_ga=2.113270336.699856249.1638893730-1586744328.1636324376)

- [PyTorch Lightning GitHub](https://github.com/PyTorchLightning/pytorch-lightning)

- [TorchMetrics](https://torchmetrics.readthedocs.io/en/latest/)

- [PyTorch Lightning Flash](https://lightning-flash.readthedocs.io/en/latest/)

- [PyTorch Lightning Bolts](https://pytorch-lightning.readthedocs.io/en/latest/ecosystem/bolts.html)
]

.pull-right[

]

---

class: inverse center middle

# Live Demonstration
